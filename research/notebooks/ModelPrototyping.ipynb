{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"../../data/raw/\"\n",
    "PROCESSED_DATA_PATH = \"../../data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(value, filename):\n",
    "    if value is not None and filename is not None:\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Please provide a valid path\".capitalize())\n",
    "    \n",
    "def load(filename):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename=filename)\n",
    "    else: \n",
    "        raise Exception(\"Please provide a valid path\".capitalize())\n",
    "    \n",
    "def clean():\n",
    "    if os.path.exists(RAW_DATA_PATH):\n",
    "        directory = os.listdir(RAW_DATA_PATH)\n",
    "\n",
    "        if os.path.isdir(RAW_DATA_PATH):\n",
    "            os.system(\"rm -rf {}\".format(os.path.join(RAW_DATA_PATH, directory[0])))\n",
    "            print(\"done\")\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Please provide a valid path\".capitalize())\n",
    "    else:\n",
    "        raise Exception(\"Please provide a valid path\".capitalize())\n",
    "    \n",
    "    if os.path.exists(PROCESSED_DATA_PATH):\n",
    "        \n",
    "        for file in os.listdir(PROCESSED_DATA_PATH):\n",
    "            os.remove(os.path.join(PROCESSED_DATA_PATH, file))\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"Please provide a valid path\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(DataLoader):\n",
    "    def __init__(self, image_path = None, batch_size = 16, image_size = 64, split_ratio = 0.25):\n",
    "        self.image_path = image_path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.split_ratio = split_ratio\n",
    "\n",
    "        self.clean_images = list()\n",
    "        self.noise_images = list()\n",
    "\n",
    "    def unzip_folder(self):\n",
    "\n",
    "        try:\n",
    "            clean()\n",
    "        except Exception as e:\n",
    "            print(\"The exception is: {}\".format(e))\n",
    "            \n",
    "        if os.path.exists(RAW_DATA_PATH):\n",
    "            with zipfile.ZipFile(self.image_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(os.path.join(RAW_DATA_PATH,))\n",
    "        else:\n",
    "            raise Exception(\"Please provide a valid path\".capitalize())\n",
    "\n",
    "    def image_transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def image_split(self, **kwargs):\n",
    "        return train_test_split(\n",
    "            kwargs[\"noise_images\"], kwargs[\"clean_images\"], test_size=self.split_ratio, random_state=42,\n",
    "        )\n",
    "\n",
    "    def create_dataloader(self):\n",
    "\n",
    "        if os.path.exists(RAW_DATA_PATH):\n",
    "            dataset = os.listdir(RAW_DATA_PATH)[0]\n",
    "\n",
    "            clean_images = os.path.join(RAW_DATA_PATH, dataset, \"clean_images\")\n",
    "            noise_images = os.path.join(RAW_DATA_PATH, dataset, \"noisy_images\")\n",
    "\n",
    "            for image in os.listdir(clean_images):\n",
    "                clean_image_path = os.path.join(clean_images, image)\n",
    "\n",
    "                if image in os.listdir(noise_images):\n",
    "                    noise_image_path = os.path.join(noise_images, image)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                self.clean_images.append(\n",
    "                    self.image_transforms()(\n",
    "                        Image.fromarray(cv2.imread(clean_image_path))))\n",
    "\n",
    "                self.noise_images.append(\n",
    "                    self.image_transforms()(\n",
    "                        Image.fromarray(cv2.imread(noise_image_path))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                image_split = self.image_split(\n",
    "                    clean_images = self.clean_images, noise_images = self.noise_images\n",
    "                    )\n",
    "\n",
    "                dataloader = DataLoader(\n",
    "                    dataset=list(zip(self.noise_images, self.clean_images)), batch_size=self.batch_size*6, shuffle=True\n",
    "                    )\n",
    "\n",
    "                train_dataloader = DataLoader(\n",
    "                    dataset=list(zip(image_split[0], image_split[2])), batch_size=self.batch_size, shuffle=True\n",
    "                    )\n",
    "\n",
    "                test_dataloader = DataLoader(\n",
    "                    dataset=list(zip(image_split[1], image_split[3])), batch_size=self.batch_size, shuffle=True\n",
    "                    )\n",
    "\n",
    "                if os.path.exists(PROCESSED_DATA_PATH):\n",
    "\n",
    "                    dump(value=dataloader, filename=os.path.join(PROCESSED_DATA_PATH, \"dataloader.pkl\"))\n",
    "\n",
    "                    dump(value=train_dataloader, filename=os.path.join(PROCESSED_DATA_PATH, \"train_dataloader.pkl\"))\n",
    "\n",
    "                    dump(value=test_dataloader, filename=os.path.join(PROCESSED_DATA_PATH, \"test_dataloader.pkl\"))\n",
    "                else:\n",
    "                    raise Exception(\"Please provide a valid path\".capitalize())\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"The exception is: {}\".format(e))\n",
    "                \n",
    "            \n",
    "            return dataloader\n",
    "\n",
    "    @staticmethod\n",
    "    def dataset_details():\n",
    "        if os.path.exists(PROCESSED_DATA_PATH):\n",
    "            dataloader = load(filename=os.path.join(PROCESSED_DATA_PATH, \"dataloader.pkl\"))\n",
    "\n",
    "            clean, noise = next(iter(dataloader))\n",
    "\n",
    "            total_data = sum(clean.size(0) for clean, _ in dataloader)\n",
    "\n",
    "            print(\"Total number of images: {}\".format(total_data))\n",
    "            print(\"Clean images shape : {}\".format(clean.size()))\n",
    "            print(\"Noisy images shape : {}\".format(noise.size()))\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Please provide a valid path\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def show_images():\n",
    "        if os.path.exists(PROCESSED_DATA_PATH):\n",
    "            dataloader = load(filename=os.path.join(PROCESSED_DATA_PATH, \"dataloader.pkl\"))\n",
    "        else:\n",
    "            raise Exception(\"Please provide a valid path\".capitalize())\n",
    "\n",
    "        plt.figure(figsize=(40, 15))\n",
    "\n",
    "        noise, clean = next(iter(dataloader))\n",
    "\n",
    "        for index, image in enumerate(noise):\n",
    "            noise_image = image.permute(1, 2, 0)\n",
    "            noise_image = (noise_image - noise_image.min()) / (\n",
    "                noise_image.max() - noise_image.min()\n",
    "            )\n",
    "\n",
    "            clean_image = clean[index].permute(1, 2, 0)\n",
    "            clean_image = (clean_image - clean_image.min()) / (\n",
    "                clean_image.max() - clean_image.min())\n",
    "\n",
    "            plt.subplot(2 * 4, 2 * 6, 2 * index + 1)\n",
    "            plt.imshow(noise_image)\n",
    "            plt.title(\"Noisy\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(2 * 4, 2 * 6, 2 * index + 2)\n",
    "            plt.imshow(clean_image)\n",
    "            plt.title(\"Clean\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(image_path=\"/Users/shahmuhammadraditrahman/Desktop/images.zip\",\n",
    "                    batch_size=4,\n",
    "                    image_size=64,\n",
    "                    split_ratio=0.25)\n",
    "\n",
    "    loader.unzip_folder()\n",
    "    dataloader = loader.create_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(Loader.dataset_details(),\"\\n\\n\")\n",
    "\n",
    "    Loader.show_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 3, image_size = 64):\n",
    "        super(DnCNN, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.padding = 1\n",
    "        self.num_repetitive = 15\n",
    "\n",
    "        self.layers = list()\n",
    "\n",
    "        self.conv_block = self.ConvBlock()\n",
    "        self.repetitive_block = self.RepetitiveBlock() \n",
    "        self.output_block = self.OutputBlock()\n",
    "\n",
    "    def ConvBlock(self):\n",
    "        self.layers = list()\n",
    "\n",
    "        self.layers.append(\n",
    "            nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.in_channels, out_channels=self.image_size, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, bias=False),\n",
    "            \n",
    "            nn.ReLU(inplace=True),\n",
    "        ))\n",
    "\n",
    "        return nn.Sequential(*self.layers)\n",
    "\n",
    "    def RepetitiveBlock(self):\n",
    "        self.layers = list()\n",
    "\n",
    "        for _ in range(self.num_repetitive):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=self.image_size, out_channels=self.image_size, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, bias=False),\n",
    "                \n",
    "                nn.BatchNorm2d(self.image_size),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ))\n",
    "\n",
    "        return nn.Sequential(*self.layers)\n",
    "    \n",
    "    def OutputBlock(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.image_size, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            residual = self.conv_block(x)\n",
    "            output = residual + self.repetitive_block(residual)\n",
    "\n",
    "            return self.output_block(output)\n",
    "        else:\n",
    "            raise Exception(\"Please provide a valid input\".capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = DnCNN()\n",
    "    print(model(torch.randn(64, 3, 64, 64)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight.data, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "    \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "def device_init(device = \"mps\"):\n",
    "    if device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    elif device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = load(filename=os.path.join(PROCESSED_DATA_PATH, \"train_dataloader.pkl\"))\n",
    "test_dataloader = load(filename=os.path.join(PROCESSED_DATA_PATH, \"test_dataloader.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = DnCNN().to(device)\n",
    "model.apply(weight_init)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epochs = 100, lr = 1e-3, device = \"mps\", display = True):\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.display = display\n",
    "        \n",
    "        self.device = device_init(device=device)\n",
    "        \n",
    "    def train(self):\n",
    "        total_train_loss = list()\n",
    "        total_test_loss = list()\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            for index, (noise_images, clean_images) in enumerate(train_dataloader):\n",
    "                noise_images = noise_images.to(self.device)\n",
    "                clean_images = clean_images.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                train_predicted = model(noise_images)\n",
    "                train_loss = criterion(train_predicted, clean_images)\n",
    "                \n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss.append(train_loss.item())\n",
    "                \n",
    "            for index, (noise_images, clean_images) in enumerate(test_dataloader):\n",
    "                noise_images = noise_images.to(self.device)\n",
    "                clean_images = clean_images.to(self.device)\n",
    "                \n",
    "                test_predicted = model(noise_images)\n",
    "                test_loss = criterion(test_predicted, clean_images)\n",
    "                \n",
    "                total_test_loss.append(test_loss.item())\n",
    "            \n",
    "            if self.display:\n",
    "                print(\"Epochs - [{}/{}] - train_loss:{} - test_loss:{}\".format(\n",
    "                    epoch, self.epochs, np.mean(total_train_loss), np.mean(total_test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(epochs=300, lr=2e-3, device=\"mps\", display=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise, clean = next(iter(test_dataloader))\n",
    "\n",
    "predicted_clean =model(noise.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 30))\n",
    "\n",
    "for index, image in enumerate(predicted_clean):\n",
    "    \n",
    "    noise_image = noise[index].permute(1, 2, 0)\n",
    "    noise_image = (noise_image - noise_image.min()) / (noise_image.max() - noise_image.min())\n",
    "    noise_image = noise_image.cpu().detach().numpy()\n",
    "    \n",
    "    plt.subplot(3 * 2, 3 * 2, 3 * index + 1)\n",
    "    plt.imshow(noise_image)\n",
    "    plt.title(\"Noisy\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    clean_image = clean[index].permute(1, 2, 0)\n",
    "    clean_image = clean_image.cpu().detach().numpy()\n",
    "    clean_image = (clean_image - clean_image.min()) / (clean_image.max() - clean_image.min())\n",
    "    \n",
    "    plt.subplot(3 * 2, 3 * 2, 3 * index + 2)\n",
    "    plt.imshow(clean_image)\n",
    "    plt.title(\"Clean\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    predicted = image.permute(1, 2, 0)\n",
    "    predicted = predicted.cpu().detach().numpy()\n",
    "    predicted = (predicted - predicted.min()) / (predicted.max() - predicted.min())\n",
    "    \n",
    "    plt.subplot(3 * 2, 3 * 2, 3 * index + 3)\n",
    "    plt.imshow(predicted, cmap=\"gray\")\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
